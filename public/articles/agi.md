# General Intelligence

> The extent to which we regard something as behaving in an intelligent manner is determined as much by our own state of mind and training as by the properties of the object under consideration. -- Alan Turing

In this article I describe the modelling of an intelligent actor broadly, connecting both humans and AGI. This is presented as a model for how actors can model other actors in their world and infer agency when the agent is not fully observed. This is not a complete model, no model can be without a proof by function, but instead is intended to be a basis by which we can discuss how general intelligences perceive and interact with their world in at an abstract enough level for the description to be applicable to approaches not yet attempted.

As a matter of clarity I use the word _actor_ when describing an intelligence under consideration and the word _agent_ when describing other agencies, human or machine, which it might interact with. We are, afterall, concerned with how AGI _actions_ effect human _agency_.

Models of the world are how an actor engages with the world around it. Despite our feelings of the wholeness and integration of our conscious experience, the human mind is composed of multiple pathways of perception which are integrated in the cortex and midbrain. While it is not known for sure, the thalamus in humans may be the seat of our entire conscious experience as it is the point of integration for the whole brain. That the whole of our experiences come to us from our senses and are created by a simulation within our brains is something we often lose perspective on as the nature of our existence.

The world, to us, feels real. But our conscious experience is not reality, it is a hallucination within our brain dreamed up by the integration of our eyes, ears, and so on. We are witness to something like reality but also far from it, the model of reality we experience within our own minds is limited by the capacity of our senses to witness the true nature of reality. Our experience is internally a model of the world.

The Free Energy Principle (FEP) has parallels to the approach here. The FEP describes the human brain as minimizing the "free energy" of predictions between its sensory input and the incoming sensory input. Another way of framing this from a machine learning perspective is that the brain evaluates a loss function over a model of its senses and the next available sensory data. The closer the brain's predictions are to its inputs the less the brain's model will change in the future.

Although this discussion is on actors with a full world model I want to be clear that this describes a fully general system which can apply to more limited actors, such as chat bots or purely digital actors. In order to make this apply scope can be removed, such as needing a model of an environment for a chat bot. For a fully general actor these models are laid out such that no scope needs to be added for the description to be complete.

## Sensors and Actuators

A transducer is a device or organ which transforms one type of energy into another. Our senses are made of transducers, such as the cells in our retinas which convert incoming photons to the electrochemical signals in neurons called action potentials.

* Sensors emit samples for their transducer type which can be used by an actor to update its model about the state of the environment.
* Actuators receive updates for their transducer type which can be used by an actor to turn manipulations of its model into changes to the environment.

We will define both sensors and actuators as coupled to an actor in a graph that originates at the actor. E.g. elbow actuators are connected to shoulder actuators and so on. An actor may have sensors and actuators for itself to estimate pose and body position in the environment as well. Sensors have an accuracy and update frequency which the actor must maintain a model of. Actuators have a control authority which the actor must maintain a model of. Some transducers may be both sensors and actuators.

## Self Model

A self model can be modeled as a directed acyclic graph (DAG) containing the sensor and actuator models. This DAG represents the attachment graph of an agent's brain(s) to its extremities. Attached to this DAG may also be separate information about the previous state. A person might internally model, for instance, where their moles are, where there tattooes are, or where there scars are. Arbitrary actors may have body plans that may be topologically toroidal or other more alien shapes, but even these should model their selves in a DAG to encode a metric of distance from the extremities and to root the brain as the origination point of the self model.

Self models are updated by the sensor network but can be manipulated without its presence. Damage to a part of the self, for instance, might be inferred from a lack of sensor data from that area. An example of a human's use of the self-model in this way is to avoid putting weight on a sprained foot to avoid the sensorial feedback of pain from that foot. Only after some period of time would tests of the foot reveal the extent of healing that has gone on, but note that the self-healing process in a person is not directly coupled to sensorial feedback. Only time and belief about the existence of the healing process have updated the self-model. 

This leads us to my next point that the human self-model incorporates higher-level belief constructs about the self. In order for the conscious mind to hold correct beliefs about the self model those beliefs must be periodically re-affirmed. Another example of this is in humans not knowing the limits of the control authority of their muscles because they have insufficiently explored the extent of the capabilities of their actuators. When a belief about capabilities is incorrect it will remain so until updated by some feedback or intellectual process. As such, this self model should include a Bayesian estimate of the likelihood of its consistency based on priors of mutability and modification processes. This is pushed down from later stages of consciousness to the self model as an update.

## Environment Models

An actor's position in its environment and kinesthetic understanding of its body pose within that environment requires it to have a model of its environment as well. This model is a series of connections between objects or landmarks in the environment and estimates the actor holds of the distances between those. No actor human or otherwise will have a cartography of its environment so accurate that it can dead-reckon without the use of landmarks to gauge distance. We say this with confidence because all actors will also have to allow for the possibility of parts of its environment to be modified outside of its knowledge, control authority, and the resolution of its sensors.

The process by which the brain encodes temporal and spatial relationships into a map of the environment is an area of growing study \cite{Conway:2021a}. These landmark based environment maps can be defined as a mesh graph of nodes and their relative positions along undirected edges. Like the self model they contain beliefs about the world which will grow inconsistent with updates from the world. The less likely a feature is to change the more likely it is to have same properties when an actor observes it again. Importantly both environmental and agentic factors can update the environment. Though an actor may know what caused an update to its self model given its own feedback it cannot know with certainty which of the changes that have occured in its environment are due to the environment itself or agents within that environment. Modeling beliefs about changes to the unobserved environment based on changes to the observed environment are an important route to the understanding an actor has on the behaviors of other agents in its world. Our ability to give directions to each other based on environment gives clues to how this is laid out in our own heads, with directions at crossroads and specific landmarks giving us instructions on how to navigate in an unfamiliar place.

Even actors without a physical presence will model the environment outside of their sensors as alien as these might be from a human point of view. A purely digital agent will observe file systems, kernel reports, and network activity just as keenly as a robot moving through a hallway. The abstract detection of other agencies in the shifting changes of text, integer, and floating point values these actors observe will be in their purest form an information theory problem related to entropy, hidden knowledge games, and bluffing.

## Attention Models

In the process of modeling the environment an actor will encounter useful objects. These objects are important enough to distinctly track within the environment model. Tools when used are briefly incorporated into an actor's self model to extend its capabilties. Humans cannot see in the dark, but with infrared cameras and displays they can and by updating the self model from a belief about those tools they can use these new sensors. This incorporation of tools into the self model can be permanent, even for people. For example, a tattoo of a ruler on the skin becomes a permenent measuring tool. These tools will transition from the attention model of the object to the self model as it is incorporated into the body image.

All actors will have an extended set of capabilties where the self model contains an understanding of how to use a tool to extend those capabilties. Language models may for instance utilize programs in similar ways to people by constructing shell commands and parsing their output. These are tools that extend their capabilties because the language model understands how to use the tool but the tool itself is not part of the model. More sophisticated digital actors will need to understand the filesystem and its permissions in order to locate and utilize tools on servers and websites.

People (at least me) tend to keep track of a few such objects on their person and have detailed mental maps of the last known locations of important tools and stores of food and water. That these objects are contained within things and move around on myself or some other actor indicate strongly my mental model of this is also a DAG of relationships which allow the translation of the entire system of containers and the objects they hold through the environment model without keeping track of individual locations. This location tracking is tied to my kinesthetic model, my somatosensory and visual senses, and my language model which allow me to compress instructions on the locations of objects into linguistic symbols expressed as strings of text such as "in my right front pocket". These have analogs in actors for items which are important to survival such as the location of all charging docks in the vicinity of a battery-powered robot.

General actors will update important locations from belief models about their environment and for particularly astute actors find ways to keep them protected or hidden if there is the possibility that their position will be disturbed before they are needed again. Some things are just too important to lose! This suggests that agents will display their preferences indirectly by the objects they pay significant attention to, so tracking objects that are not of direct importance to an actor may be important in the revealed preferences and detection of other agents. And other agents and expectations of agentic behavior take more attentional resources than other parts of the environment as agents may modify the environment themselves outside of an actor's model.

More generally individual objects and tools can be said to exist within an attention model that is overlaid on the world model. The higher the importance score of a region of the environment the more likely it is an actor to be attentive to changes in that part of its model. The attention of an agent is a behavioral presentation of its attention model on the world. Resources, objects, tools, and even sensory experiences can all be part of an actor's attention model.

## Belief Models

Finally (and most important) are an actor's models of its own internal priorities, states, and beliefs. In people these are carried forward as an ego story which describes the past and future heading of an individual's life course. These repeated stories are an explanation both of how we got to where we are and why we are doing what we currently are to get to where we are going. The stories of our lives we tell are the collection of our goals, past events, and beliefs about ourselves we carry through the world, all informed by our internal models of that world and ourselves. Any actor which has goals will have directed action. The story of that actor's path through an environment even told from an outside perspective will inform us about our beliefs about another agent's beliefs, creating an internal agentic model of that other agent.

This will also work in reverse where an actor hypothesizing agentic behavior from us will produce a story that describes our inferred goals both in the short and longer term. "He went to get a cup of water" is a goal informed about knowledge of human priorities and the heirarchy of needs we display in reaching longer-term objectives. To understand the goal of getting even a cup of water requires enough context about human needs and a model of the sensory experience to create a short story of actions: "He went to get a cup of water because he is thirsty. He is thirsty because he is dehydrated. A cup of water will satisfy his goal of maintaining homeostasis." You might not be aware of all the logical leaps made from a simple statement, but that is because people have shared implicit context.

The modeling of internal state most requires communication as an update mechanism to the beliefs an actor holds about the goals of other actors. Maybe the cup of water was not meant to hydrate but to clear the throat or to be given to another who needed water. It is a significant source of complexity in the expansion of the social environment to include deception and delusion. An actor that can communicate its goals to you is definitely agentic, but those goals are now the subject of skepticism and belief inference. When one's existence may be on the line in the face of competition or adversarial goals this will be the most important modeling an actor performs in maintaining and expanding its strategic position in the world.

## Connections

Each of these models represents a predictor for information from the previous step in the forward pass and a predictor of responses from the next step in the backward pass. For example, to know where objects are in an environment an actor must place its sensors within its self model and then feed that information forward to the environment model. If an actor has eyes its self model informs the world model where those messages are positioned on its body and oriented in space. The environment model takes information from the self model about what is changing around it to encode the new environment. The attention model reflects changes in the environment with where the attentional objects in that environment which then integrate into the beliefs which contain the large body of intelligence of the agent about the meanings and symbolic presentations of those objects.

In the backward pass similar predictions are being made in each model stage. The attention model updates its attentional weights prior to receiving feedback from the beliefs. If tracking a ball through the air, the attentional model may include information about objects that may intersect the ball, such as a bat, based on prediction of what the later model will request attention on. Feedback comes from both sides: when the environment presents unexpected things to the attention those are _surprising_ if they were not predicted. The difference between the model's estimate in the next step and updates it receives are simultaneously worth including as extra encodings to the next step and are feedback to the current step that the model weights require updates.

#### Figure 1
![Figure 1](/images/tikz_model.png)

Each model in these stages have identical update criteria with the exception of the self model. The self model contains extra updates for the actuators as there exists a tight loop between the actuator output and the desired activation of the actuator. This is part of balance, posture, and strength when the actuator is near the limits of its control authority. In the human brain it is believed the cerebellum is responsible for much of the feedback control over the muscles of the body \cite{Bastian:2011a}.

The self model is also the largest part of the human brain by volume. This would encompass the cerebellum and the auditory, olfactory, motor, somatosensory, and visual cortices. Since the compositional range for interpretation of each sense is so large a significant amount of neural architecture is devoted to transforming these into something that can be presented to the integrated consciousness. Though I have presented each layer here linearly in the brain these heirarchies are present in _cortical columns_ across the cortex where the levels of prediction are duplicated many times and may cross regions or skip levels. For this model of intelligence I will ignore the intricacies of how the brain actually functions and treat these complications as implementation details.

The state of the self model at a time step $t$ is given as the sum over all sensors for that time transformed into model space plus the priors in the model from the last time step.

$$M(t) = \sum_{i} M_s S_{i}(t) + M(t-1)$$

The expected state of the world is the transformation of the model into world space.

$$ E[W(t)] = W_M * M(t) $$

Models are used for predictions prior to sensor feedback. The efficacy of a model is in it being carried forward in time. The error in the model is the difference between its prediction and the next observed world state.

$$ M_{err}(t) = E[W(t)] - W_M * M_{pred}(t) $$

Similar equations are expected of each model stage for which the model predicts the feed forward and feedback predictions and passes them along to the next stage. The model also updates its internal weights based on the loss of these expectations from the reply.

## Example Architecture

For a sketch of how this might be implemented in an architecture, let's use the popular transformer layout as a base. It's been shown that transformers are excellent at solving general tasks, are designed to be efficient computationally, and have been shown to be data effecient in training. For these reasons this architecture, or something very similar to it, will continue to be used well into the development of AGI.

Transformers can be used as autoencoders if their inputs are encoded and their outputs are decoded and then compared. In Figure 2 I have used an autoencoder layout where the output of a model layer is compared to its input from the current time step, giving an error signal on how well the model is representing the next sample it receives. Just as predicting the next token has worked well in generating excellent language models it's likely that predicting just the next time step with enough samples and a large enough model will be enough to encode significant information about the temporal dynamics of the world.

Also in Figure 2 I have distinguished the environment model by attaching memory about the environment to this stage of the transformer blocks and using an embedding of that environment history to feed into the model at each stage so that it can use its previous experiences to learn the environment. These outputs are then saved into the memory region through the decoder to keep them in a sensor / self space representation.

Finally, the layout of the attention and belief models here is as many separable components which may be used in unison or separately to create complex beliefs about the world. This is similar to the human neocortex which can specialize regions to learn specific uses of tools or representations of the world and its dynamics. The models are all connected at the belief layer so that the beliefs can freely communicate.

The gradient signal through this network is at each time step, as such there are significant optimizations that will need to be implemented in this design for it to be feasible. I recognize that this is a computationally challenging task, infeasible in this layout, and there are many subtle details that are left out for this to be a complete design, it is used only for demonstration purposes. One possible improvement, for instance, would be for the attention models to play a role in the optimization and gradient finding processes, such as selecting parts of the models to optimize at some given time based on attention. Another possible approach would be a roving weight update attention which inspects the gradient of the model at places which have the highest error, updating only parts of the entire architecture at one time through training approaches to focus learning on one area while inference goes on elsewhere.

#### Figure 2
![Figure 2](/images/example_implementation.jpg)
