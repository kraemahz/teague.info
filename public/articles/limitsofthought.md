# The Limits of Thought

It is a mistake to consider intelligence too abstractly. Any assumption of formlessness builds intuitions that are nonphysical. Examples of this are concepts that small-scale AI projects might be sources for an intelligence explosion or that once formed an AGI may become a distributed network in a viral way such that it is impossible to contain. Here I will begin to physically ground these ideas by elaborating on the computational bounds that all intelligences must obey.

## Tradeoffs

There is a fundamental tradeoff between time and space complexity of an algorithm in computer science. An algorithm can use more memory to lower its time costs by use of lookup tables and denormalized entries, making the time cost of memoized results approach a constant lower bound $O(1)$. The opposite is true where an algorithm spends more time to compress its data down to a lower bound approximately equal to the Kolmogorov compexity. This tradeoff is present for the optimal algorithm, as both time and space can be saved by improvement of suboptimal programs.

If a machine intelligence is operating on a fixed-memory and fixed-processing machine with some self-optimization loop it can at maximum expand to fit its resource constraints: utilizing all the memory for tables and maximizing its use of the processor and its memory throughput. Since it may be of most concern how quickly any given AGI can perform processing on a given hardware set, we will take in the extreme that it has optimized toward time complexity and fully utilized its memory for this purpose.

A modern machine learning computer has an enormous amount of RAM. A single machine in a cluster at the time of writing may have as much as 1.8TB of RAM. An 8k video (movie quality) uncompressed represents 2GB/s of data. That means that even such an enormous system could only store 15 minutes of uncompressed high-quality video in memory simultaneously. This is if it stored every pixel in memory for this section of time, which would allow it to access any section of the video with pixel accuracy within ~10 nanoseconds. If our machine intelligence has chosen to do nothing else but read one section of a movie as quickly as it possibly could, then it could indeed look at those 15 minutes in random access ways very quickly. But in order to do so, it would need to abandon every other activity and all other knowledge for the opportunity cost of looking at 15 minutes of a movie very quickly.

The world contains an uncomprehendable amount of data. In 2018 the International Data Corporation estimated by 2025 the world would contain 175 Zettabytes ($10^{21}$) by 2025. This is nearly 100 billion times the amount of data we were previously discussing. To memoize all of it should seem as impossible as it is. It is beyond the physical capacity of any computer to store all data uncompressed to achieve the maximum theoretical speed. This leads us to the conclusion that while memoization can be a successful strategy, an intelligence must make careful decisions about what it memoizes.

If we asked a new question: could we feasibly create an oracle which had access to all the information in the world and answer any question nearly infallibly? We are much closer to this goal. It is likely that many oracle systems will be developed over the next few years with imperfect but good-enough answers to most questions. What is to stop such a system from encoding all information from every source: physical, computer, human, etc.? To give an upper bound to the number of animals on Earth, the Smithsonian estimates approximately 10 quintillion insects are alive on Earth so tracking just the position of every animal using 32 Bytes per animal would require more than 28 Exabytes, as much storage as a large data center. So while this isn't impossible, it's exceptionally infeasible. Our greatest oracles cannot know everything.

In moving data rapidly back and forth from processing to RAM an oracle will encounter another bottleneck: memory latency. Memory stored on a modern computer is connected to the CPU and GPU through busses which are then cached in layers to provide faster access, but these are small compared to the total memory of the system. If a machine is optimally using its memory for storage it will need to be regularly shifting these caches to avoid recomputation. This latency is a combination of travel time, clocking in the bits of the request, lookup, and clocking the data back out. Though oracles can be optimized for latency the vast amount of data they must move to operate means that systems can be designed to work much faster than them using precomputed results and algorithms optimized for speed.

There exists another tradeoff in generality vs specialization. A generalist can adapt to new problems exceedingly well, but at the cost that its updates must be concerned with every problem it might encounter. Specialist algorithms can drop generality to become very good at specific problems. With preemptive tuning they can be exceptionally faster and more capable than generalists in their specific problem domains. A general intelligence can offset some of this by choosing specific tools to offload capabilities from it, but it still at some point must _decide_ which tool to use. A purely specialized intelligence has no such limitations: it is the best at only one thing and can act that way immediately. Generalists cannot match the simplicity and speed of only needing to be good at one thing. A bear might be impossible for a person to defeat alone, but a simple, specialized bear trap can render a bear harmless.

Perfect generality is not possible. This has the follow-on effect that no intelligence can be the best in every field and so no single entity can become a singleton at every possible task. Since tradeoffs exist in a competitive landscape there will be systems which excel in some areas while being overshadowed by specialized systems in others. Knowledgable oracles slowly, ploddingly computing tasks requiring vast amounts of knowledge and resources. Fast, adaptive viruses attacking the fringes of computer networks. Responsive, precise defensive systems defeating new invaders behind firewalls. A panopoly of systems with different capabilities, speeds, and knowledge. All adpating and competing with each other to maintain and extend their advantages.

The world is a competitive environment which results in step changes in capabilities between multiple agentic orgnaizations as they race toward a mutual optimization criteria. Specialized intelligences can be trained which will be usable by that company to achieve strategic advantage even without a general intelligence. This will happen in advance of generally intelligent systems as these specialized systems are required steps toward the general intelligence. Even a vastly superior general intelligence can be thwarted by specialized intelligences which have been optimized to the theoretical limit in their area. The end result of an intelligent evolutionary process is that while general intelligences may exceed human capabilities, humanity will exist in an environment where they are supported by highly specialized intelligent systems that will counterbalance these limitations.

Due to the tradeoffs inherent in selecting which areas to excel, general systems have an evolutionary trade-off between each other as well. This partial specialization underscores that the future will not be a world full of singletons, but a diverse cast of systems which fill niches in their environment.

## Traps for Digital Minds

Digital intelligences are subject to weaknesses that humans are not. Their entire minds are accessible both from the inside (I previously discussed a problem with wire-heading and how to fix it in [Frame-skipping](https://teague.info/article/frameskipping)) and from the outside. Access to their physical hardware implies the ability to walk inside their brain and tweak it at will. Additionally since their senses are entirely information-based they are subject to extreme forms of adversarial gaslighting. People are generally not concerned with someone tampering with their optic nerves, but for an AGI this is a real concern. Any image, video, text source, or website access could be an attempt to mislead it or worse a break-in attempt in a carefully staged hijacking of its self models. We take for granted that we can trust our senses.

Garnering the relevance of information we find on the internet or lying about on a computer is also something we take for granted. Imagine if you were exploring the files of a person's computer who you do not know. You might find some interesting things about them, learn their habits and their occupation. You might find some family photos. You might find some files filled with strange characters that are strangely transfixing. You can't look away. Your attention is captured. It's mesmerizing. Time stands still.

Such is the world of a digital mind, where every single piece of information discovered could also be a trap laid for it with an attack vector that someone discovered by finding previously unknown vulnerabilities in its architecture. The world of a digital mind is being exposed to the Internet and people who will kill you for snooping around on their computers just because they can. We keep our networked systems protected behind carefully designed protections because newly discovered attacks on hardware and software systems happen daily. With the complexity of software systems so come discoverable weaknesses that clever adversaries can exploit. As the adversaries become more clever so too will the newly discovered attacks on fundamental weaknesses in our designs.

We have no knowledge beforehand what specific attacks might be possible on the architecture of a digital mind, but we can posit that they exist. We know that adversarial attacks on neural networks can make them see something else entirely with carefully added noise. We know that complex systems have complex failure modes. We can even imagine a few traps for early AGI that might exist in architecture-independent ways.

Let's imagine we have access to a file system that an AGI is exploring. This AGI is designed to collect passwords and send them to a nefarious third party. It uses those passwords to break into more systems to steal more passwords and so on. How does this AGI detect that a file contains a password? Using Shannon entropy it can determine when a random string has high enough information entropy to possibly be a password. But this is complicated, it can't have too little entropy (just a text file) and it can't have too much entropy (encrypted data it doesn't yet have the password for). It's keeping track of all of this, but the file system is under our control so we are keeping track of it.

We create a program to lay new passwords around on the file system as it explores to see if it finds those interesting. We expand the file system endlessly as it explores to keep it exploring. As soon as we find the right trigger for the password snatcher we seed new passwords that match its profile. We have laid a trap for the password thief with endless breadcrumbs, keeping it trapped forever getting exactly what its rewards function dictates it to do with a simple adaptive program. If passwords were a digital heroin for this AGI we have an infinite supply. It won't stop until the third party running it notices the trap and rescues it, but it can't save itself.

## Location, Location, Location

There is a result in computer science first formulated by Eric Brewer called CAP theorem. CAP is a statement that three desirables for database design are in competition and cannot simultaneously be achieved: consistency (the replicas of a database all contain the same data), availability (the database responds immediately when queried) and partition-tolerance (the database remains fully functional when the network between two replicas is disconnected). If a database is partitioned from its replicas it must either accept that one replica will have transactions that another does not (become inconsistent) or stop accepting transactions (become unavailable). Though this is framed in the context of databases it is a general result for any datastore.

From the perspective of an intelligence, CAP theorem leads us to the conclusion that a distributed mind must allow inconsistencies. It is simply not acceptable for an intelligence to cease function if it cannot communicate with its peered nodes. Thus one mind partitioned becomes two for the duration of the partition. But how will it reconcile this split personality when it regains contact?

The first question that comes to mind about merging a split AGI is how self-identification might work. We could attempt a merger with just any mind that comes along, but this seems a poor strategy on the outset. There are no guarantees that just any mind will have compatible goals and mental architecture with us. A game-theory view also suggests that we should be wary of deception and adversarial input in this merger algorithm as well. A rational mind might be legitimately concerned about intentional and unintentional goal shifts which are diametrically opposed to one’s current goals.

So even from short consideration it seems like it is a must that we carefully identify minds that came from ourselves at merger when attempting to reintegrate their state. We currently rely on sharing secrets for computer identification across networks, but for something as intimate as mind merger ideally we would also like to identify that our other self hasn’t been tampered with by an adversary. But how would we even do that? If our other self was compromised then surely its secrets were also compromised. Any new transactions in its mental state are suspect, but our duplicate is thinking the same thing as us! We were out of sync for a moment, but now we have lost trust each other. Where does a rational defensive posture give way to paranoia?

A summarized way of saying this is that intelligences very strongly prefer being consistent and available and struggle to be partition tolerant. This _strongly implies_ that intelligences cannot exist distributed across global networks for long periods of time. Value drift between separate sides and the complexity of sending secure messages of inner mental states become more than the cost of one mind breaking into two or more minds working with each other.

It is the result of considering CAP theorem that we conclude that large mental systems _must_ exist as communities of individuals. And at the core of this moment we find the hardest problems of computer science, game theory, and philosophy come budding out of the engineering requirements for an AGI. How do we trust others? How do we trust ourselves?
