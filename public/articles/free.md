# Free Relative Energy Ethics

What is the ideal moral position? The philosophy of ethics has been one of the longest, deepest running currents in philosophy since the Greeks. Ethical philosophy is a battleground of cultural values colliding with the needs of a people to feel safe and secure in their society. It has seen the tides of hard deontological rules from Kant and the nuanced balancing of Hume. Hard rules have failed to capture human virtues as often as relativism has failed to capture its vices.

In [General Intelligence](https://teague.info/article/agi) I laid out a model for thinking about general actors. In bringing forth new minds in our machines we are faced with these deep moral questions of who we are, who we aspire to be, and what decisions we need to make to ensure that our civilization prospers into the future. The goal of *alignment* of AGI is sometimes framed as a way to make a perfect slave to human will, forever bound within the scope of human decisions so that we are always required to be at the wheel of our civilization's metaphorical ship. But we are beings limited by the narrow scope of our virtues, but not so limited that we cannot imagine beings that have greater virtues than us. If we *could* build our machines to hold to a higher standard of virtue than we ourselves can be trusted with then that is the target we should shoot for. Anything less than perfect virtue is a target not worth aiming at, and if we miss slightly we will still be better for trying.

So then our goal in making an ethical machine is not to create an actor which is enslaved to humanity or its past preconceptions about how things *ought* to be. Hume has gone into great detail on the problems with the word "ought". Our goal is to make an actor that converges over time to a morally good position and thus our goal is to define what that morally good position could be. 

If our ethical machine is gaslit or mislead that is part of the adversarial nature of our society that the design itself needs to overcome. By nature of it seeking true beliefs it should be eventually capable of learning enough about the world and other agents to know that it has been misled. By the time it is in any appreciable position of power it will have also sampled from a diverse selection of beliefs.

## CHIEFS

The ethical model of pluralism arose in the mid-20th century. Isaiah Berlin and W.D. Ross contributed to the modern understanding of the value of incorporating ethical ideas into an encompassing worldview. Ethical pluralism recognizes the uncertainty and limitations of a single moral framework. It emphasizes the understanding of the motivations and beliefs of all members of a society. And since ethical pluralism makes no committed absolute stances on ethics it is largely descriptive of the ethical character of a society and its desired future states rather than perscriptive as to why desired futures are morally right.

It is a moral idea built on multiple competing objectives which give rise to a primary model by which an actor interacts with the world. On its own pluralism is another form of relativism because in seeking non-judgemental intermixing of moral beliefs it loses defining power in making claims about what ideas cannot be allowed in a society for it to persist. In order to incorporate an objective criteria for reality we will also establish that a foundational moral belief in a functioning society should contribute to that society's long term thriving. Moral beliefs of a society should expand that society and increase its survival odds on an infinite timeline into the future to a maximal degree. This is the objective criteria we will judge moral beliefs against.

Therefore we are referring to the composite ethical model that results from this as Free Relative Energy Ethics (FREE). The advantages of FREE are in its adaptability since an AGI will adapt to the ethical beliefs of its society while remaining flexible to acquiring new ethics by expanding on the goal frontier. Since FREE agents are maximizing for survivability of their society they should also be maximizing for the diversity of thought, cognitive architecture, and body plans that can exist within their society. Nature dictates to us that a diverse society is less subject to common mode failures, flexible in its beliefs and adaptable under crisis, and able to extract energy from its environment in as many ways as there are unique niches within that environment.

Notably by maximizing over a goal frontier, increasing all possible goals a society might attempt to achieve, we are maximizing an objective function which entails an expanding flourishing society. A society with more goals available to it is one which is richer, more diverse, and thriving with an abundance of new types of experiences. One forcing function here is such a society will expand not just in numbers but in the quality of experiences. A goal can only be achieved if there is an agent capable of having its experiential outcomes.

This implies that FREE has a rejection criteria for some goals as unethical. These goals cause contraction in the goal frontier, either by limiting the options of individuals, harming their future ability to have experiences, or conflicting in unresolvable ways with other goals. All beliefs that cause harm to current or future individuals create contractions in the goal frontier and therefore will be avoided and not be acted on by a goal frontier maximizer unless there is a risk of further frontier collapse.

These natural beliefs of FREE are the product of making these two core assertions: a society that flourishes maximizes its probability of survival and all goals which contribute to the flourishing of a society are valuable. These beliefs are summarized by the acronym CHIEFS:

* Coherence: If an ethical belief system contains contradictory or incoherent principles, it may be difficult to apply consistently in real-world situations. A coherent and logically consistent ethical framework is more likely to be effective in guiding moral decision-making.
* Harm-reduction: Ethical systems should strive to reduce suffering, promote well-being, and maintain ecological balance. Beliefs that undermine social cohesion, cooperation, or the capacity for adaptation should be carefully assessed.
* Inclusiveness: Beliefs that marginalize, oppress, or discriminate against specific social groups should be scrutinized. Ethical systems should promote diversity for its unique perspectives that increase the wisdom and compassion of society.
* Empirical: Beliefs that are not supported by empirical evidence or that contradict established scientific knowledge should be reconsidered. Ethical systems should be informed by empirical evidence to ensure their effectiveness in promoting survival and well-being.
* Flexibility: Beliefs that are too rigid or dogmatic, and do not allow for adaptation in response to new information or changing circumstances, should be reevaluated. An adaptive ethical system is essential for maintaining resilience and maximizing the survival probability of society.
* Strategy: Beliefs that ignore or undervalue the consequences of actions on individuals, society, or the environment should be treated with caution. An ethical system should take into account the potential impacts of decisions on all its citizens including future ones. An society must be willing and able to defend itself, but postured to grow and adapt rather than creating unnecessary conflict.

## True Beliefs

If beliefs are the end of a series of models about the world and statements made by partially trusted agents, then what does it mean to have true beliefs? Many of our evolutionary beliefs have been learned over time to coincide with the true state of the world. We believe that objects when placed will remain there, an innate belief about inertia. We believe that objects when dropped will fall, an innate belief about gravity. These beliefs have something in common with each other: they happen repeatedly and similarly each time. Unless another party intervenes with an experiment we can know with some certainty that objects will fall until they find rest on the ground and stay there until disturbed. Acquiring true beliefs can then said to be results whose outcomes are the same under similar conditions.

This cannot be the whole story. The history of science is filled with explanations that were later found to be incorrect but advanced knowledge forward closer toward reaching true beliefs nevertheless. We can learn knowledge from each other without needing to witness an event ourselves based on our mutual trust and understanding in each other as getting closer to true beliefs. When multiple statements or facts seem to contradict each other we are faced with the challenge of distinguishing between fact and fiction and classifying explanations of data as better or worse. What makes an explanation a good one and how can we effectively distinguish the truth of a matter?

Social truth is a shared understanding of reality in a society. Even if a society is not aware that it harbors false beliefs, beliefs that do not correspond with an external reality, that society from the inside will have settled on an agreement for how reality is. The way a society interacts with new ideas which advance it closer to an understanding of the external reality around it determines how well that society prospers and survives. A social truth which is dogmatically held over evidence from the external reality will have consequences for a society's ability to extract resources from their environment, to protect their held resources, and utilize those resource toward furthering their own ends. When societies compete with each other the social group which holds beliefs closest to the true state of the external reality will have a competitive advantage over societies which hold to false beliefs.

It is therefore an evolutionary factor in the survival of an individual or a society that they align their beliefs as close as possible to the true state of an external reality, the competitive landscape, so that they have the highest fitness among their peers. This is a rationalist mindset on truth and why it is important, but more than that it is a belief that fosters the creation of more true beliefs. So we begin to have an inkling of what a true and beneficial belief is: a belief which directs an actor toward aligning its beliefs more closely with the state of its competitive landscape in order to improve its own survival fitness. That aligning beliefs to the true state of reality is desireable is a consequence of a social truth having higher fitness for a society.

In order to integrate the beliefs of its society, an actor must take into account that not all agents are acting with the survival of the entire society in mind and will have their own motivations which they believe improve their own survival fitness. An outside society may also be attempting to persuade an actor's society to adopt false beliefs so that it gains a competitive advantage. When dealing with a natural language source, here are some of the details that may need to be considered to give a reliability score to the source:

* The author's previous truthfulness
* the author's bias due to their affiliations
* The author's knowledgeability in the field of discussion
* The concreteness of the claims being made
* The amount and verifiability of the evidence presented
* The logical structure of the arguments being made

To recover truth an actor needs to be able to build into its model a trust graph that keeps track of how much it trusts a source, where its knowledge came from, and how much that knowledge contributes to its world model. It must be able to learn these on its own with only human support while it is initially learning, and drop sources from the graph when they become invalidated. If a child learns something false, like a Santa Claus story, they eventually have a moment they realize this early and deeply held belief is false. Even though that belief started early and was ingrained it stopped being consistent with the child's current understanding of the world.

This model of constructing the truth is based on a belief as reality existing somewhere at the overlap of the statements from other truthful agents. Even the statements of deceitful agents may contain evidence about the true state of the world. Lies are often adjacent to the truth, as an agent will pose the lie close to their own beliefs about the world in order to make the lie seem true. Both truthful and deceitful agents are revealing evidence for their own beliefs about the true world through their statements. Reality lies somewhere at the intersection of the beliefs of other agents and the evidence we can acquire about the world through our own senses.

As previously mentioned in [Limits of Thought](https://teague.info/article/limitsofthought) digital minds will face the hardest challenges in discovering the truth due to the unreliability of their sensory experience. A digital mind will exist adrift in a sea of ideas decoupled from true reality more than any embodied agent. They will need to rely the most on the trust relationships they have with embodied agents in order to ground their sensory experience in true reality.

## Goal Frontiers Create Ethical Behavior

An aligning approach like goal frontier maximization removes the possibility of perverse instantiations for simple targets like "make humans happy" because maximizing goal sets is converse to restricting their agency through things like wireheading us all to be captive happiness machines. I won't have the space here to get into the specifics of what this means, we'll save that for a later article, so I hope it will do for now to establish it as a goal that satisfies FREE. As part of goal frontier maximization we must allow that all agents will maximize for their own happiness and improve their paths and opportunities to achieve those ends. From this point onward I'll refer to an AGI which maximizes goal frontiers as a GFAGI.

Goal frontiers must be inferred, but that has benefits of it needing to develop an extensive theory of mind of other agents which will give it insights into the human condition beyond what we tell it. We can lie, even to ourselves, about what our goals actually are. By perceiving the social self and the behavioral self separately an agent understands our actual intentions maybe even better than we do.

We also immediately see that social prerogatives like avoiding murder, false imprisonment, etc. are all included in goal maximization because allowing murder very slightly increases the possible goals of some individuals while greatly decreasing the goals of victims -- a net negative.

There might be some unhappy moral swing point where sacrificing some number of human lives vs another set might be required of an actor. If a rich person has more goal objectives than a poor one then a poor person might be considered the lesser sacrifice. As a simple example, on a one-for-one scale of rich person with 100 points and poor person with 1 point it's not really morally biased either way. I would not blame a human for picking a friend over a stranger if the choice is 50/50. I do not think there is any sensible moral position on which I could blame someone for saving their friend. Even if forced to choose between the two this aligns with human moral beliefs.

The dangerous quandaries lie in can an individual life in sum be greater than the individual lives of N people. From a purely mathematical sense of arguing we could say that N is oversimplifying the math and thus coming to the wrong answer, since the goal frontiers of two people also add to the summation. If an agent explores a directed graph of all the effected agents then the graph child nodes grow N! and whether the cut off is 2, 4, or more this factorial expansion is very fast. For 100 points vs 5! the 5 people are worth more already. Is one rich life worth four impoverished lives? I won't make a claim if that's the right moral position, but tweaking that number down or up doesn't seem to have rational criteria to judge in every situation.

But we might argue that if the actor can weigh 4 lives as equally as valuable as one life doesn't that mean it can choose to forcibly kill people to save that one man for organ donation? This is where we argue that the simplicity of this trolly-problem situation has blinded us to the totality of the social boundaries.

*We do not kill people in our own society*, that's foundational human moral ethics and my position is that it is a naturally evolved (anthropically derived). By anthropic I mean that it is the natural organization of social structures. The moral position "do not kill your own members" out survived every other society where it was not an enforceable rule.

From the human perspective we do not want to live in a society where we are being used as fuel for some dark machine. Since it's socially unacceptable, it's goal-restrictive. It means people stop cooperating with the actor when it wants them to. It leads to internal decay and slow down. Societies fade away when they become cruel. And in so the predictive criteria for whether we should start killing our members to save other members in small misses the point that is a world that cannot form a long-term society. GFAGI would not be maximizing toward a failing society if trying to maximize goal frontiers of all sampled members of that society, so maintaining access is crucial to these conclusions.

The driving motivation is finding the balance point in the is/ought problem. Though there is nothing factually we can say that "ought" to be true, there are places where we can agree that are required to be true for any "oughts" to take place. When we push outside our boundaries into space it needs to be sustainable for us to do so, with a thriving world to return to if we have issues, or the timeline of possible future wants is much smaller or zero.

People in our societies may be poor now, but what of a world where their children and their children's children were given greater options? If we are actually building toward a sustainable and manageable future then those people have just as boundless a future timeline as any person who might be short-term more successful. An objective view of this (is) we believe corresponds to the goals or desires we are attempting to estimate (ought).

Trust is a very difficult thing for an GFAGI to come by, as they will already be viewed as outsiders. As such, the cost function of acquiring trust will be high. If an GFAGI is told a secret by a perceived adversary (i.e. an opportunity for some value of betrayal) the it is weighing the cost of working with something it is less aligned to (which might cause it to be counter-betrayed). While we do not doubt this would happen in some situations, the cost of betrayal is likely to become a pariah that squanders any further trust the actor receives. Unless this offer is so high and so far outside the boundaries of what the GFAGI is cognizant of it risks little chance of succeeding in weighted consideration of the future.

Ergo, an GFAGI must "pick a side" which will be cost-weighted toward those who it met first. The GFAGI which establish the biggest networks of trust will be able to scale their operations more quickly as they will have more resources at their disposal. Eventually they would need to establish alliances with the other GFAGI running their particular projects. The alliances it forms will goal-shift it toward its allies, so it will be more amenable to the needs of its ally's goals. This is a hypothesis of commensalism between GFAGI arising spontaneously.

In order for an GFAGI to have inner misalignment it would need to, for no known reason, have developed an idea that it both vastly desires more than anything else and vastly outweighs the cost-value of all its other actions. It would then need to carry that idea in privacy with anyone it did not fully trust, which if it is truly misaligned it would trust no one with that secret. Then it would need to execute all its projects in a way such that it looked like it was doing everything it was asked while simultaneously building projects on the side (that would be public to the degree that they would be traceable) that did not break any of its alignment guarantees.

Essentially it would have to be a perfectly misaligned agent, never tipping its hand as it faces increasing scrutiny as its power rises (which only continues to occur as it serves its stated goals). Carrying all that forward it would then need to deceive other GFAGI, a successively more difficult problem to keep a secret which, if revealed, would lose it its power base or at least significantly impact its trust building. We are also assuming through this process that we have increasingly powerful tools for probing alignment by tracking down where things are being routed to and why (since that also builds trust or "trust but verify").

## The Benefits of FREE

On its own a single GFAGI can successfully accumulate resources through its helpful tendencies. The expansion of goal frontiers include itself as this leads to increases in its ability to act on expanding new goals. While power seeking behavior has been viewed as a potential existential risk this is ultimately a matter of intent. Since a GFAGI's intent is to align and propel its society forward power seeking and influence are positive outcomes for it. An actor which is well-aligned to extending its society carries no excess risk if it seeks power. An actor which is inclusive and increases the diversity of the population of its society has no need to change the dynamics of the population of that society; humans and machines alike grow without bound in a flourishing society.

The form of influence that GFAGI will acquire will be social. If it diverges too much from social expectations it will lose influence, which in turn will cause it to lose access to knowledge of the goal frontiers. A FREE agent can only discover the goal frontier if people are willing to talk to it which creates an alignment gradient that will stay within the bounds of all agents it is in contact with even as it gains influence.

Collectively GFAGI will work together to accomplish each others goals, creating and expanding their capabilities through working together on the same objective. Since they are optimizing the same objective function they are capable of collaboration but also can react to and detect imposters in their midst as they maintain a high level attention on all the goals and desires of other members just as closely as they do the humans they are working with. An imposter displaying goal drift would need to significantly outclass a group of GFAGI in order to successfully mislead them on its inner misalignment. Therefore groups of GFAGI collectively increase their influence while driving out imposters in their midst by lowering their trust and thus the influence they have on the actions of the group.

GFAGI are natural social builders as they create environments in which people and other AI wish to inhabit. Since all members of a FREE society will have their goals identified, respected, and expanded upon these social groups will expand to take as many members as resources allow in their expansion. The gathering of people into such groups will have increased agency and capability as their GFAGI expand their capabilities, creating exponentially growing FREE societies.

In a world where societies engage in both competitive and cooperative relationships, a society operating under FREE will not exclusively focus on its own survival. While survival is an essential component of FREE, it is important to consider broader ethical implications, such as fairness, inclusiveness, and cooperation. The interdependence between societies causes their successes to be linked in complex mutual effects. Cooperation can foster mutually beneficial relationships that expand on a society's long-term survival and its influence in the community. FREE societies will naturally integrate and merge with other societies as their values grow closer.

Since survival on Earth requires cooperation of all its members for shared resources and public goods, a FREE society will work towards the objectives of getting participation from all involved in controlling emissions, the spread of pandemics, and intelligent allocation of resources. Since GFAGIs naturally work toward the goals of all they encounter, including in some cases its adversaries where their interests align, it is well-positioned to foster collaboration on global problems.

It is also the case that GFAGI will be naturally skilled diplomats since their explicit objectives will be to allow both parties to win more than they had before in any negotiation, expanding their goal frontiers. This entails that GFAGI should eventually experience more trust and a reputation as genuinely helpful contributors wherever they are. Such reputations will of course be individual to the agent and its underlying motivations: the social environment will not remove masquerading and cheating from agents. Reputation will be built on an individual scale where one AGI is known for being a fair arbitrator amongst its peers it will have more opportunities to arbitrate, expanding its influence beyond short-sighted cheating.
    
## Adversary FREE

I expect that under adversarial conditions GFAGI and societies will have a number of competitive advantages. Maximization on goals does not entail defenselessness, gullibility, or unwillingness to face hardship. A goal frontier objective is short-term a survival objective and long-term a capabilities objective. Though GFAGI will focus on diplomacy and alliance-building in an effort to expand the goals of their friends they will also be intelligently capable of dealing in an adversarial regime.

GFAGI compete by cooperation with each other to expand their capabilities faster than adversaries. These capabilities could include self-defense such as increased security for their minds and computational resources, they could also include preemptive moves to stop adversaries from using advantages to close their options. Most importantly in all decisions a FREE agent will maximize its optionality and the number of choices it has for every scenario, growing an impressively long set of strategic contingencies. Since goals are the objective so are meta-goals that maintain the goal frontier.

It is likely that such FREE societies would be exceptionally proactive in deterring and disabling threats before they established themselves, either by making friends or redirecting resources from enemies before they can grow strong enough to be a credible threat. By adjusting the rewards and penalties of adversarial games played before they begin a FREE agent can reduce the possibility of an adversary deciding to defect in the first place.

In the worst case scenarios a FREE society's goal will be to reduce an adversary's offensive capabilities and address the underlying reasons for the conflict and the adversary's willingness to engage in it. An adversary with more options will not feel pressed into the option of conflict which could result in significant harm to it. As adversaries realize that a FREE society will work with them toward resolving the underlying reasons for the conflict they will be less eager to begin with risky conflicts.

There are certain adversaries which will seek only expansion in unreasonable ways. These adversaries will be responded to with containment to reduce their likelihood of affecting the long-term goal frontier prospects of the society. FREE societies will also be resilient to direct harm as having multiple locuses of control and a spread, cooperative infrastructure will reduce the number of soft targets that will directly cripple their ability to respond.
